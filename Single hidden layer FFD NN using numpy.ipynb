{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "predict the test score for the given value of study hours and sleeping hours "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation of a single hidden layer FFD NN using numpy array \n",
    "\n",
    "\n",
    "#### Type of problem - Regression    \n",
    "\n",
    "Architecture:\n",
    "\n",
    "hidden layers - 1\n",
    "\n",
    "no. of neurons:\n",
    "\n",
    "    input layer-2\n",
    "\n",
    "    hidden layer-3\n",
    "\n",
    "    output layer-1\n",
    "\n",
    "activation function - sigmoid (input and hidden)\n",
    "\n",
    "Weight initialisation type - Random init\n",
    "\n",
    "Bias - No bias taken as there is only one hidden layer and number of inputs are less\n",
    "\n",
    "Back propogation - gradient descent\n",
    "\n",
    "epochs - 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = (hours studying, hours sleeping), y = score on test\n",
    "x = np.array(([2, 9], [1, 5], [3, 6], [5, 10]), dtype=float) # input data as float for calculations\n",
    "y = np.array(([92], [86], [89]), dtype=float) # output data as float for calcualtions\n",
    "\n",
    "# Normalization\n",
    "xAll = xAll/np.amax(xAll, axis=0) # scaling input data\n",
    "y = y/100 # scaling output data (max test score is 100)\n",
    "\n",
    "# split data\n",
    "X = np.split(xAll, [3])[0] # training data\n",
    "xPredicted = np.split(xAll, [3])[1] # testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the Neural_Network() class, functions for forward pass and backward pass are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Neural_Network():\n",
    "\n",
    "#function for forwrd pass and backward pass\n",
    "  def train(self, X, y):\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "\n",
    "    \n",
    "  def __init__(self):\n",
    "#number of neurons in each layer\n",
    "    self.inputSize = 2\n",
    "    self.outputSize = 1\n",
    "    self.hiddenSize = 3\n",
    "\n",
    "#weights attached between layers\n",
    "\n",
    "# (3x2) weight matrix from input to hidden layer\n",
    "    self.W1 = np.random.randn(self.inputSize, self.hiddenSize)\n",
    "# (3x1) weight matrix from hidden to output layer    \n",
    "    self.W2 = np.random.randn(self.hiddenSize, self.outputSize) \n",
    "\n",
    "#forward propagation through our network   \n",
    "\n",
    "#Matrix multiplications using dot product function\n",
    "\n",
    "  def forward(self, X):\n",
    "    \n",
    "    self.z = np.dot(X, self.W1) \n",
    "    self.z2 = self.sigmoid(self.z) # activation function for neurons to fire output to hidden layer\n",
    "    self.z3 = np.dot(self.z2, self.W2) \n",
    "    o = self.sigmoid(self.z3) # final activation function for neurons to fire output to output layer\n",
    "    return o #output\n",
    "\n",
    "# activation function for neuron firing\n",
    "  def sigmoid(self, s):\n",
    "    return 1/(1+np.exp(-s))\n",
    "\n",
    "#sigmoid prime is function to find derivative, which helps in building GD algorithm in the end for back propagation\n",
    "  def sigmoidPrime(self, s):\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Backward propagation through the network to correct the weights\n",
    "  def backward(self, X, y, o):\n",
    "    \n",
    "    self.o_error = y - o # error in output, differene in actual and predicted\n",
    "    self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
    "\n",
    "    self.z2_error = self.o_delta.dot(self.W2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
    "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
    "\n",
    "#adjusting weights\n",
    "    self.W1 += X.T.dot(self.z2_delta) \n",
    "    self.W2 += self.z2.T.dot(self.o_delta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "0.00011420068108275547\n",
      "# 500\n",
      "\n",
      "Input (scaled): \n",
      "[[0.4 0.9]\n",
      " [0.2 0.5]\n",
      " [0.6 0.6]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.90943738]\n",
      " [0.87437517]\n",
      " [0.88506162]]\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "for i in range(501): #500 epochs\n",
    "  \n",
    "  NN.train(X, y)\n",
    "\n",
    "#After 500 epochs, loss and outputs     \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss    \n",
    "print(\"# \" + str(i) + \"\\n\")\n",
    "print(\"Input (scaled): \\n\" + str(X))\n",
    "print(\"Actual Output: \\n\" + str(y))\n",
    "print(\"Predicted Output: \\n\" + str(NN.forward(X)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
